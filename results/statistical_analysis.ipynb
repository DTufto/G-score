{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-28T12:24:30.592612Z",
     "start_time": "2024-10-28T12:24:26.519377Z"
    }
   },
   "source": [
    "from scipy.stats import ttest_rel\n",
    "import pandas as pd\n",
    "import math"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T12:24:30.600059Z",
     "start_time": "2024-10-28T12:24:30.592612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_FTTSE(energy, time, is_training=True):\n",
    "    \"\"\"\n",
    "    Calculate the FTTSE (Function of Time To Solution and Energy) metric.\n",
    "    :param energy: Energy consumption in Joules\n",
    "    :param time: Time to solution in seconds\n",
    "    :param is_training: Whether the calculation is for training or inference\n",
    "    :return: FTTSE value\n",
    "    \"\"\"\n",
    "    if is_training:\n",
    "        return energy * (time**2)\n",
    "    else:\n",
    "        return energy * math.exp(time)\n",
    "\n",
    "def calculate_G_score_per_trial(data, number_of_inferences, alpha):\n",
    "    \"\"\"\n",
    "    Calculate the G-score for each model within each trial in the data.\n",
    "    :param data: DataFrame containing the data\n",
    "    :param number_of_inferences: Number of inferences to consider\n",
    "    :param alpha: Performance weight (0 <= alpha <= 1)\n",
    "    \"\"\"\n",
    "    for trial_number in data['Trial'].unique():\n",
    "        trial_data = data[data['Trial'] == trial_number].copy()  # Work with a copy for safe modification\n",
    "\n",
    "        # Calculate combined and normalized FTTSE\n",
    "        trial_data['FTTSE_combined'] = (1 - trial_data['FTTSE_Training']) + (1 - trial_data['FTTSE_Inference']) * number_of_inferences\n",
    "        fttse_min, fttse_max = trial_data['FTTSE_combined'].min(), trial_data['FTTSE_combined'].max()\n",
    "        trial_data['FTTSE_norm'] = (trial_data['FTTSE_combined'] - fttse_min) / (fttse_max - fttse_min)\n",
    "\n",
    "        # Calculate G_Score for each model within the trial\n",
    "        for model in trial_data['Model'].unique():\n",
    "            model_data = trial_data[trial_data['Model'] == model]\n",
    "            f1 = model_data['F1_Score'].values[0]\n",
    "            energy = model_data['FTTSE_norm'].values[0]\n",
    "            beta = 1 - alpha\n",
    "            g_score = alpha * f1 + beta * energy\n",
    "\n",
    "            # Assign the G_Score value to the original data\n",
    "            data.loc[(data['Trial'] == trial_number) & (data['Model'] == model), 'G_Score'] = g_score\n",
    "\n",
    "\n",
    "def sign_test(x, y):\n",
    "    return 2 * min(ttest_rel(x, y).pvalue, 0.5)"
   ],
   "id": "d7503d2506c2e450",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T12:24:30.628097Z",
     "start_time": "2024-10-28T12:24:30.600059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"data/presentation.csv\")\n",
    "\n",
    "df['FTTSE_Training'] = df.apply(\n",
    "    lambda row: calculate_FTTSE(row['Training_Energy_Joules'], row['Training_Time_Seconds'], is_training=True), axis=1)\n",
    "df['FTTSE_Inference'] = df.apply(\n",
    "    lambda row: calculate_FTTSE(row['Inference_Energy_Joules'], row['Inference_Time_Seconds'], is_training=False), axis=1)\n",
    "calculate_G_score_per_trial(df, 1000, 0.5)\n",
    "\n",
    "models = df['Model'].unique()\n",
    "\n",
    "print(df)"
   ],
   "id": "6640773d76aaf001",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Trial                Model  Training_Time_Seconds  Training_Energy_Joules  \\\n",
      "0      1        Decision Tree                 260.61                 4085.22   \n",
      "1      1          Naive Bayes                   8.26                  102.34   \n",
      "2      2        Decision Tree                 262.69                 4258.20   \n",
      "3      2          Naive Bayes                   8.19                  103.03   \n",
      "4      3        Decision Tree                 261.91                 4219.94   \n",
      "5      3          Naive Bayes                   8.26                  105.70   \n",
      "6      1  Logistic Regression                  24.36                  772.60   \n",
      "7      2  Logistic Regression                  23.97                  728.63   \n",
      "8      3  Logistic Regression                  24.25                  678.65   \n",
      "\n",
      "   Inference_Time_Seconds  Inference_Energy_Joules  F1_Score  FTTSE_Training  \\\n",
      "0                    0.04                     0.69    0.5544    2.774582e+08   \n",
      "1                    1.25                    18.05    0.5760    6.982413e+03   \n",
      "2                    0.04                     1.05    0.5576    2.938415e+08   \n",
      "3                    1.25                    17.91    0.5760    6.910851e+03   \n",
      "4                    0.05                     0.74    0.5599    2.894746e+08   \n",
      "5                    1.26                    18.65    0.5760    7.211657e+03   \n",
      "6                    0.11                     3.16    0.6081    4.584683e+05   \n",
      "7                    0.11                     2.86    0.6081    4.186423e+05   \n",
      "8                    0.11                     2.99    0.6081    3.990886e+05   \n",
      "\n",
      "   FTTSE_Inference   G_Score  \n",
      "0         0.718159  0.277200  \n",
      "1        63.000690  0.788000  \n",
      "2         1.092851  0.278800  \n",
      "3        62.512042  0.788000  \n",
      "4         0.777941  0.279950  \n",
      "5        65.749111  0.788000  \n",
      "6         3.527439  0.803343  \n",
      "7         3.192555  0.803450  \n",
      "8         3.337671  0.803481  \n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T12:24:30.650544Z",
     "start_time": "2024-10-28T12:24:30.628097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "RQ: How can we define a measure for evaluating machine learning models that takes into account both energy and performance?\n",
    "\n",
    "H0: There is no difference in G-scores among the classification models (and is therefore not a useful metric).\n",
    "H1: There is a difference in G-scores among the classification models (and is therefore a useful metric).\n",
    "\"\"\"\n",
    "\n",
    "sign_test_results_g_score = pd.DataFrame(index=models, columns=models)\n",
    "for model1 in models:\n",
    "    for model2 in models:\n",
    "        if model1 == model2:\n",
    "            sign_test_results_g_score.loc[model1, model2] = (0, 0, 1.0, \"Fail to reject\")\n",
    "        else:\n",
    "            data1 = df[df['Model'] == model1]['G_Score'].values\n",
    "            data2 = df[df['Model'] == model2]['G_Score'].values\n",
    "            \n",
    "            p_value = sign_test(data1, data2)\n",
    "            accept = \"Fail to reject\" if p_value > 0.05 else \"Reject\"\n",
    "\n",
    "            sign_test_results_g_score.loc[model1, model2] = (p_value, accept)\n",
    "\n",
    "print(sign_test_results_g_score)"
   ],
   "id": "9b9f9bb9cfb0bbe2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Decision Tree  \\\n",
      "Decision Tree            (0, 0, 1.0, Fail to reject)   \n",
      "Naive Bayes          (4.901613574096284e-06, Reject)   \n",
      "Logistic Regression  (4.159125753224131e-06, Reject)   \n",
      "\n",
      "                                          Naive Bayes  \\\n",
      "Decision Tree         (4.901613574096284e-06, Reject)   \n",
      "Naive Bayes               (0, 0, 1.0, Fail to reject)   \n",
      "Logistic Regression  (1.4580099291385602e-05, Reject)   \n",
      "\n",
      "                                  Logistic Regression  \n",
      "Decision Tree         (4.159125753224131e-06, Reject)  \n",
      "Naive Bayes          (1.4580099291385602e-05, Reject)  \n",
      "Logistic Regression       (0, 0, 1.0, Fail to reject)  \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T12:24:30.672211Z",
     "start_time": "2024-10-28T12:24:30.651725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "RQ1: What is the energy efficiency, measured in FTTSE, of selected classification models (NaÃ¯ve Bayes, Logistic Regression,\n",
    "Decision Tree, and Support Vector Machine) during training when applied to the Microsoft's Cats vs Dogs dataset?\n",
    "\n",
    "H0a: There is no difference in energy efficiency among the classification models during training.\n",
    "H1a: There is a difference in energy efficiency among the classification models during training.\n",
    "\"\"\"\n",
    "\n",
    "sign_test_results_training = pd.DataFrame(index=models, columns=models)\n",
    "for model1 in models:\n",
    "    for model2 in models:\n",
    "        if model1 == model2:\n",
    "            sign_test_results_training.loc[model1, model2] = (0, 0, 1.0, \"Fail to reject\")\n",
    "        else:\n",
    "            data1 = df[df['Model'] == model1]['FTTSE_Training'].values\n",
    "            data2 = df[df['Model'] == model2]['FTTSE_Training'].values\n",
    "            p_value = sign_test(data1, data2)\n",
    "            accept = \"Fail to reject\" if p_value > 0.05 else \"Reject\"\n",
    "            \n",
    "            sign_test_results_training.loc[model1, model2] = (p_value, accept)\n",
    "            \n",
    "            # print(f\"{model1} vs {model2}: {n_pos} positive, {n_neg} negative, p-value = {p_value}, {accept} the null hypothesis H0a\")\n",
    "            \n",
    "print(sign_test_results_training)"
   ],
   "id": "d3e5174e5762bade",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Decision Tree  \\\n",
      "Decision Tree            (0, 0, 1.0, Fail to reject)   \n",
      "Naive Bayes          (0.0005826531803877175, Reject)   \n",
      "Logistic Regression  (0.0005878274873670021, Reject)   \n",
      "\n",
      "                                         Naive Bayes  \\\n",
      "Decision Tree        (0.0005826531803877175, Reject)   \n",
      "Naive Bayes              (0, 0, 1.0, Fail to reject)   \n",
      "Logistic Regression   (0.003499942864720424, Reject)   \n",
      "\n",
      "                                 Logistic Regression  \n",
      "Decision Tree        (0.0005878274873670021, Reject)  \n",
      "Naive Bayes           (0.003499942864720424, Reject)  \n",
      "Logistic Regression      (0, 0, 1.0, Fail to reject)  \n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T12:24:30.695283Z",
     "start_time": "2024-10-28T12:24:30.672722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "RQ2: What is the energy efficiency, measured in FTTSE, of selected classification models (NaÃ¯ve Bayes, Logistic Regression,\n",
    "Decision Tree, and Support Vector Machine) during inference when applied to the Microsoft's Cats vs Dogs dataset?\n",
    "\n",
    "H0b: There is no difference in energy efficiency among the classification models during inference.\n",
    "H1b: There is a difference in energy efficiency among the classification models during inference.\n",
    "\"\"\"\n",
    "\n",
    "sign_test_results_inference = pd.DataFrame(index=models, columns=models)\n",
    "for model1 in models:\n",
    "    for model2 in models:\n",
    "        if model1 == model2:\n",
    "            sign_test_results_inference.loc[model1, model2] = (0, 0, 1.0, \"Fail to reject\")\n",
    "        else:\n",
    "            data1 = df[df['Model'] == model1]['FTTSE_Inference'].values\n",
    "            data2 = df[df['Model'] == model2]['FTTSE_Inference'].values\n",
    "            p_value = sign_test(data1, data2)\n",
    "            accept = \"Fail to reject\" if p_value > 0.05 else \"Reject\"\n",
    "\n",
    "            sign_test_results_inference.loc[model1, model2] = (p_value, accept)\n",
    "\n",
    "            # print(f\"{model1} vs {model2}: {n_pos} positive, {n_neg} negative, p-value = {p_value}, {accept} the null hypothesis H0b\")\n",
    "\n",
    "print(sign_test_results_inference)"
   ],
   "id": "78c3e847277f0758",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Decision Tree  \\\n",
      "Decision Tree            (0, 0, 1.0, Fail to reject)   \n",
      "Naive Bayes          (0.0005781806676582863, Reject)   \n",
      "Logistic Regression   (0.013792409162570152, Reject)   \n",
      "\n",
      "                                         Naive Bayes  \\\n",
      "Decision Tree        (0.0005781806676582863, Reject)   \n",
      "Naive Bayes              (0, 0, 1.0, Fail to reject)   \n",
      "Logistic Regression   (0.000554567004837298, Reject)   \n",
      "\n",
      "                                Logistic Regression  \n",
      "Decision Tree        (0.013792409162570152, Reject)  \n",
      "Naive Bayes          (0.000554567004837298, Reject)  \n",
      "Logistic Regression     (0, 0, 1.0, Fail to reject)  \n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T12:24:30.698826Z",
     "start_time": "2024-10-28T12:24:30.695283Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b4a65481849b6cee",
   "outputs": [],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
